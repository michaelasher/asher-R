d$UV_num = ifelse(d$UV_v_No=='UV',.5,-.5)
d$FB_num = ifelse(d$FB_v_No=='Feedback',.5,-.5)
# Standardizing Psychological Outcomes
d$UVpostVidTotal_Z = (d$UVpostVidTotal - mean(d$UVpostVidTotal))/sd(d$UVpostVidTotal)
d$ExpPostVidTotal_Z = (d$ExpPostVidTotal - mean(d$ExpPostVidTotal))/sd(d$ExpPostVidTotal)
d$UVpostWriTotal_Z = (d$UVpostWriTotal - mean(d$UVpostWriTotal))/sd(d$UVpostWriTotal)
d$UVfinalTotal_Z = (d$UVfinalTotal - mean(d$UVfinalTotal))/sd(d$UVfinalTotal)
d$ExpPostWriTotal_Z = (d$ExpPostWriTotal - mean(d$ExpPostWriTotal))/sd(d$ExpPostWriTotal)
d$UVfinalTotal_Z = (d$UVfinalTotal - mean(d$UVfinalTotal))/sd(d$UVfinalTotal)
d$SIfinalTotal_Z = (d$SIfinalTotal - mean(d$SIfinalTotal))/sd(d$SIfinalTotal)
d$CurrentGPA_Z = (d$CurrentGPA - mean(d$CurrentGPA))/sd(d$CurrentGPA)
d$SIpostVidTotal_Z = (d$SIpostVidTotal - mean(d$SIpostVidTotal))/sd(d$SIpostVidTotal)
d$DT_z = scale(d$DT) %>% as.numeric()
d$BreakTaken_c = d$BreakTaken - .5
d$Effort_z = scale(d$EffortStudying) %>% as.numeric()
d$NumAtt_z = scale(d$NumAtt) %>% as.numeric()
d$NC_z = scale(d$NC) %>% as.numeric()
d$BaselineScore_z = scale(d$BaselineScore) %>% as.numeric()
# Does DT predict score?
m1 = lm(totalScore ~ DT_z, data = d)
devtools::load_all()
vTable(m1)
?as.tibble
?kable
vTable(m1)
setwd('../../../../Desktop/myfirstpackage/')
devtools::load_all()
vTable(m1)
?dplyr
??dplyr
library(dplyr)
?%>%
??broom
??magritr
devtools::document()
devtools::document()
vTable(m1)
devtools::load_all()
vTable(m1)
vTable(m1,m1)
vTable(m1)
vTable(m1)
vTable(m1)
devtools::load_all()
devtools::document()
vTable(m1)
devtools::load_all()
vTable(m1)
vTable(m1,m1)
devtools::load_all()
devtools::document()
devtools::load_all()
vTable(m1)
devtools::load_all()
vTable(m1)
devtools::load_all()
vTable(m1)
vTable(m1)
devtools::load_all()
devtools::load_all()
vTable(m1)
packdir = getwd()
library(lmSupport)
library(rstudioapi)
library(tidyverse)
library(cowplot)
library(psych)
library(knitr)
library(kableExtra)
library(xtable)
library(car)
library(effects)
library(lm.beta)
#### Set Working Directory ####
currentPath = getActiveDocumentContext()$path
setwd(dirname(currentPath))
rm(currentPath)
getwd()
setwd("../FYP_Data")
#### Functions ####
options(knitr.kable.NA = '')
judyTable = function(..., Betas = TRUE, ModelStats = FALSE, twoColumns = TRUE){
unstand = list(...)
stand = list(...)
elipsis = list(...)
l = length(unstand)
for(i in seq(1:l)){
stand[[i]] = lm.beta(unstand[[i]])
}
if(Betas){
elipsis = stand
} else{
elipsis = unstand
}
l = length(elipsis)
c = summary(elipsis[[1]])$coefficients %>% data.frame() %>% rownames_to_column() %>% data.frame()
if(Betas){c = select(c, -starts_with('Est'))}
for(i in seq(from = 1, to = l)){
if(i > 1){
m = summary(elipsis[[i]])$coefficients %>% data.frame() %>% rownames_to_column() %>% data.frame()
if(Betas){m = select(m, -starts_with('Est'))}
c = merge(c,m, by = 'rowname', all.x = TRUE, all.y = TRUE)
}
cNames = paste(rep(c('est','SE','t','p'),i),seq(2:ncol(c)), sep = '.')
colnames(c)[2:ncol(c)] = cNames
}
c$Missing1 = 0
c$Missing2 = 0
c$Missing3 = 0
c$Missing4 = 0
c$Missing5 = 0
c$Missing6 = 0
c$Missing7 = 0
c$Colon = str_count(c$rowname, ":")
if(l > 1){
c$Missing1 = ifelse(is.na(c$est.1),1,0)
}
if(l > 2){
c$Missing2 = ifelse(is.na(c$est.5),1,0)
}
if(l > 3){
c$Missing3 = ifelse(is.na(c$est.9),1,0)
}
if(l > 4){
c$Missing4 = ifelse(is.na(c$est.13),1,0)
}
if(l > 5){
c$Missing5 = ifelse(is.na(c$est.17),1,0)
}
if(l > 6){
c$Missing6 = ifelse(is.na(c$est.21),1,0)
}
if(l > 7){
c$Missing7 = ifelse(is.na(c$est.25),1,0)
}
c = arrange(c, Missing1, Missing2,Missing3,Missing4,Missing5,Missing6,Missing7,Colon)
c = select(c, -starts_with('Miss'), -starts_with('Colon'))
if(twoColumns){c = select(c, -starts_with('SE'), -starts_with('t'))}
if(Betas){c = filter(c, rowname != '(Intercept)')}
c = as.tibble(c)
cNames = rep(c('B','S.E','t','p'), l)
if(twoColumns){cNames = rep(c('B','sig.'),l)}
colnames(c)[2:ncol(c)] = cNames
# Removing Names of Factors from Variable Names
facs = select_if(elipsis[[l]]$model, is.factor)
for(i in seq(from = 1, to = ncol(facs))){
if(colnames(facs)[i] %in% as.character(levels(facs[,i]))){
c$rowname = sub(paste(colnames(facs)[i],colnames(facs)[i],sep = ""), colnames(facs)[i], c$rowname)
}
else{
c$rowname = sub(colnames(facs)[i], "", c$rowname)
}
}
c$rowname = gsub(":", " x ", c$rowname)
if(ModelStats){
r2list = rep(NA,2*l+1)
obslist = rep(NA,2*l+1)
for(i in seq(from = 1, to = l)){
r2 = summary(elipsis[[i]])$r.squared
nobs = nobs(elipsis[[i]])
r2list[2*i] = r2
obslist[2*i] = nobs
}
c = rbind(c, obslist)
c[nrow(c),1] = 'Observations:'
c = rbind(c, r2list)
c[nrow(c),1] = 'R-Squared:'
}
# Change first column name to say 'DV: ____'
colnames(c)[1] = paste("DV: ",colnames(elipsis[[1]]$model)[1],by="")
kable(c, digits = 3, format = 'html', booktabs = F) %>% kable_styling()
}
# x is a matrix containing the data
# method : correlation method. "pearson"" or "spearman"" is supported
# removeTriangle : remove upper or lower triangle
# results :  if "html" or "latex"
# the results will be displayed in html or latex format
corstars = function(x, method=c("pearson", "spearman"), removeTriangle=c("upper", "lower"),
result=c("none", "html", "latex","kable"), decimals=2){
#Compute correlation matrix
require(Hmisc)
x <- as.matrix(x)
correlation_matrix<-rcorr(x, type=method[1])
R <- correlation_matrix$r # Matrix of correlation coeficients
p <- correlation_matrix$P # Matrix of p-value
## Define notions for significance levels; spacing is important.
mystars <- ifelse(p < .001, "***", ifelse(p < .01, "** ", ifelse(p < .05, "*  ", ifelse(p < .1, "    ", "    "))))
## trunctuate the correlation matrix to two decimal
R <- format(round(cbind(rep(-1.11, ncol(x)), R), decimals))[,-1]
## build a new matrix that includes the correlations with their apropriate stars
Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
diag(Rnew) <- paste(diag(R), " ", sep="")
rownames(Rnew) <- colnames(x)
colnames(Rnew) <- paste(colnames(x), "", sep="")
## remove upper triangle of correlation matrix
if(removeTriangle[1]=="upper"){
Rnew <- as.matrix(Rnew)
Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
Rnew <- as.data.frame(Rnew)
}
## remove lower triangle of correlation matrix
else if(removeTriangle[1]=="lower"){
Rnew <- as.matrix(Rnew)
Rnew[lower.tri(Rnew, diag = TRUE)] <- ""
Rnew <- as.data.frame(Rnew)
}
## return the correlation matrix
if (result[1]=="none") kable(Rnew, format = 'html', booktabs = T) %>% kable_styling()
else{
if(result[1]=="html") print(xtable(Rnew), type="html")
else print(xtable(Rnew), type="latex")
}
}
#### Make Combined Data ####
# :::: Read Part 1
p1 = read_csv('May6Part1.csv', col_types = cols(
PCMbase1 = 'i',
PCMbase2 = 'i',
PCMbase3 = 'i',
YrsMath = 'i',
CurrentGPA = 'd',
CurrentYearInSchool = 'i',
MathInterestGeneric1 = 'i',
MathInterestGeneric2 = 'i',
ConfPreLearning = 'i',
UVforMath1 = 'i',
UVforMath2 = 'i',
Clarity = 'i',
SIpostVid1 = 'i',
SIpostVid2 = 'i',
SIpostVid3 = 'i',
ExpPostVid1 = 'i',
ExpPostVid2 = 'i',
UVpostVid1 = 'i',
UVpostVid2 = 'i',
UVpostVid3 = 'i',
`Practice1Timer_Click Count` = 'i'
)) %>%
filter(grepl('^1', ID)) %>%
filter(!grepl('^p9',ID)) %>%
filter(ID>1052) %>% rename('Practice1Clicks' = `Practice1Timer_Click Count`)
# :::: Read Part 2
p2 = read_csv('May6Part2.csv', col_types = cols(
SIpostWri1 = 'i',
SIpostWri2 = 'i',
SIpostWri3 = 'i',
ExpPostWri1 = 'i',
ExpPostWri2 = 'i',
UVpostWri1 = 'i',
UVpostWri2 = 'i',
UVpostWri3 = 'i',
`Practice2Timer_Click Count` = 'i',
Percept1 = 'i',
Percept2 = 'i',
Percept3 = 'i',
Percept4 = 'i'
)) %>% rename('Practice2Clicks' = `Practice2Timer_Click Count`)
# :::: Read Part 3
p3 = read_csv('May6Part3.csv', col_types = cols(
DT = 'i',
NC = 'i',
NI = 'i',
ST = 'i',
CV1 = 'i',
CV2 = 'i',
SIfinal1 = 'i',
SIfinal2 = 'i',
SIfinal3 = 'i',
IntInLearningMore = 'i',
PCFinal1 = 'i',
PCFinal2 = 'i',
ExpFinal1 = 'i',
UVfinal1 = 'i',
UVfinal2 = 'i',
UVfinal3 = 'i',
StudyPercep1 = 'i',
StudyPercep2 = 'i',
StudyPercep3 = 'i',
StudyPercep4 = 'i'
)) %>%
filter(grepl('^1', ID)) %>%
mutate(
NumAtt = NC + NI
) %>%
rename('TestBreakTime' = `BreakTime_Page Submit`)
p3$TestBreakTime = as.numeric(p3$TestBreakTime)
# :::: Scoring tests, adding total score and test scores to the third data frame
testItems = read_csv('May6Part3.csv') %>%
select(ID, starts_with('Set'), -contains('timer')) %>%
filter(grepl('^1', ID)) %>%
filter(!grepl('^p9',ID))
testItems[, c(2:61)]  <- sapply(testItems[, c(2:61)], as.numeric)
key1 = c(198,204,294,989,620,516,275,319,936,682,
473,756,520,312,533,588,2132,946,1008,403,
969,2501,2201,609,225,648,1554,1680,1656,1624)
key2 = c(860,966,384,612,154,888,495,867,429,276,
984,1200,273,793,483,775,1736,1136,1680,777,
728,336,840,912,3393,945,646,1148,4212,1504)
s1 = score.multiple.choice(key1, testItems[,c(2:31)], score = FALSE, total = TRUE, missing = FALSE)
s2 = score.multiple.choice(key2, testItems[,c(32:61)], score = FALSE, total = TRUE, missing = FALSE)
p4 = cbind(s1,s2,p3)
p3$test1Score = rowSums(p4[,1:30], na.rm = TRUE)
p3$test2Score = rowSums(p4[,31:60], na.rm = TRUE)
p3$totalScore = p3$test1Score + p3$test2Score
p3$test1NI = rowSums(p4[,1:30]==0, na.rm = TRUE)
p3$test2NI = rowSums(p4[,31:60]==0, na.rm = TRUE)
p3$testTotalNI = p3$test1NI + p3$test2NI
p3$testPercentage = (p3$totalScore)/(p3$totalScore + p3$test1NI + p3$test2NI)
# :::: Create Variable Scores
# :: Baseline Perceived Competence in Mathematics (PCM)
p1$PCMTotal = varScore(p1, Forward = c('PCMbase1','PCMbase2','PCMbase3'),
Range = c(0,7),
MaxMiss = .4)/3
# :: Baseline generic interest in math
p1$GenericIntTotal = varScore(p1, Forward = c('MathInterestGeneric1','MathInterestGeneric2'),
Range = c(0,7),
MaxMiss = .6)/2
# :: Baseline UV for math
p1$UVforMathTotal = varScore(p1, Forward = c('UVforMath1','UVforMath2'),
Range = c(0,7),
MaxMiss = .6)/2
# :: Situational Interest after the video
p1$SIpostVidTotal = varScore(p1, Forward = c('SIpostVid1','SIpostVid2','SIpostVid3'),
Range = c(0,7),
MaxMiss = .4)/3
# :: Expectancies after the video:
p1$ExpPostVidTotal = varScore(p1, Forward = c('ExpPostVid1','ExpPostVid2'),
Range = c(0,7),
MaxMiss = .6)/2
# :: UV Perceptions after the video:
p1$UVpostVidTotal = varScore(p1, Forward = c('UVpostVid1','UVpostVid2','UVpostVid3'),
Range = c(0,7),
MaxMiss = .4)/3
# :: Situational Interest after writing/feedback:
p2$SIpostWriTotal = varScore(p2, Forward = c('SIpostWri1','SIpostWri2','SIpostWri3'),
Range = c(0,7),
MaxMiss = .4)/3
# :: Expectancies for success after writing/feedback:
p2$ExpPostWriTotal = varScore(p2, Forward = c('ExpPostWri1','ExpPostWri2'),
Range = c(0,7),
MaxMiss = .6)/2
# :: UV Perceptions after writing/feedback:
p2$UVpostWriTotal = varScore(p2, Forward = c('UVpostWri1','UVpostWri2','UVpostWri3'),
Range = c(0,7),
MaxMiss = .4)/3
# :: Competence Valuation before test:
p3$CVTotal = varScore(p3, Forward = c('CV1','CV2'),
Range = c(0,7),
MaxMiss = .6)/2
# :: Situational Interest final:
p3$SIfinalTotal = varScore(p3, Forward = c('SIfinal1','SIfinal2','SIfinal3'),
Range = c(0,7),
MaxMiss = .4)/3
# :: Percieved Competence for the technique:
p3$PCfinalTotal = varScore(p3, Forward = c('PCFinal1','PCFinal2'),
Range = c(0,7),
MaxMiss = .4)/2
# :: Percieved Competence for the technique:
p3$ExpFinalTotal = varScore(p3, Forward = c('ExpFinal1','SIfinal2','SIfinal3'),
Range = c(0,7),
MaxMiss = .4)/3
# :: Perceptions of UV final:
p3$UVfinalTotal = varScore(p3, Forward = c('UVfinal1','UVfinal2','UVfinal3'),
Range = c(0,7),
MaxMiss = .4)/3
p3$Feedback_v_No = ifelse(p3$CN == 3 | p3$CN == 4, 1, 0)
p3$UV_v_No = ifelse(p3$CN == 2 | p3$CN == 3, 1, 0)
# ::: Merge Data
d = merge(p1, p2, by.x = 'ID', by.y = 'ID')
d = merge(d, p3, by.x = 'ID', by.y = 'ID')
# ::: Exclude participants
exclusions = c(1064,1105,1109,1113,1126,1253,1292,1299,1064,
1107,1126,1168,1204,1299, 1348, 1362, 1325)
d = filter(d, as.numeric(ID) > 1055)
d = filter(d, !(ID %in% exclusions))
d$Female = as.factor(ifelse(d$Q107_2 == 'cis female' |
d$Q107_2 == 'female'|
d$Q107_2 == 'Female'|
d$Q107_2 == 'female/woman'|
d$Q107_2 == 'woman'|
d$Q107_2 == 'women', 'Female', 'Male'))
contrasts(d$Female) = varContrasts(d$Female, Type = 'POC', POCList = list(c(1,-1)), Labels = 'Female_vs_Male')
table(d$UV_v_No)
table(d$Feedback_v_No)
d$UV_v_No = factor(d$UV_v_No, levels = c(0,1), labels = c('NoUV','UV'))
contrasts(d$UV_v_No) = varContrasts(d$UV_v_No, Type = 'POC',  POCList = list(c(-1,1)), Labels = 'UV_vs_NoUV')
d$FB_v_No = factor(d$Feedback_v_No, levels = c(0,1), labels = c('NoFeedback','Feedback'))
contrasts(d$FB_v_No) = varContrasts(d$FB_v_No, Type = 'POC', POCList = list(c(-1,1)), Labels = 'FB_vs_NoFB')
d$BreakTaken = ifelse(d$DT > 0, 1, 0)
d$Age = as.numeric(ifelse(d$Q107_1 == 'sophomore', 19, d$Q107_1))
d$PriorKnowl = varRecode(d$PriorKnowlYesNo, c(1,2), c(1,0))
d$IntInFollowup = d$Q157
d$UseInFuture = d$Q153
d$D2L_OtherTechniques = d$Q155
d$ST = ifelse(is.na(d$ST), 0, d$ST)
d$WritingHelpful = d$Percept1
d$WritingDifficulty = d$Percept2
d$InstructionsClear = d$Percept3
d$StudyTimeUtil = d$StudyPercep1
d$EffortStudying = d$StudyPercep2
d$PreparedForTest = d$StudyPercep3
d$EnoughStudyTime = d$StudyPercep4
d$PropCorrect = d$NC/d$NumAtt
d = select(d, ID, Female, Age, CN, UV_v_No, FB_v_No, DT, ST, BreakTaken, NC, NI, NumAtt,
test1Score,test2Score,totalScore,
Feedback_v_No, YrsMath, CurrentGPA, CurrentYearInSchool,
ConfPreLearning, Clarity,
PCMbase1, PCMbase2, PCMbase3, PCMTotal,
UVforMathTotal, UVforMath1, UVforMath2,
MathInterestGeneric1, MathInterestGeneric2, GenericIntTotal,
SIpostVid1, SIpostVid2, SIpostVid3, SIpostVidTotal,
ExpPostVid1, ExpPostVid2, ExpPostVidTotal,
UVpostVid1, UVpostVid2, UVpostVid3, UVpostVidTotal,
SIpostWri1, SIpostWri2, SIpostWri3, SIpostWriTotal,
ExpPostWri1,ExpPostWri2,ExpPostWriTotal,
UVpostWri1, UVpostWri2, UVpostWri3, UVpostWriTotal,
CV1, CV2, CVTotal,
SIfinal1, SIfinal2, SIfinal3, SIfinalTotal,
IntInLearningMore,
PCFinal1, PCFinal2, PCfinalTotal,
ExpFinal1,
UVfinal1, UVfinal2, UVfinal3, UVfinalTotal,
PriorKnowl, IntInFollowup, UseInFuture, D2L_OtherTechniques,
WritingHelpful, WritingDifficulty, InstructionsClear,
StudyTimeUtil, EffortStudying, PreparedForTest, EnoughStudyTime,
testPercentage, testTotalNI, Practice1Clicks, Practice2Clicks, Percept1,
Percept2, Percept3, Percept4, TestBreakTime, PropCorrect
)
d[!complete.cases(d),] # Missing data from 1115, 1154, 1187, 1217, and 1292, 1316, 1320
d = na.omit(d)
# Merge Baseline Scores
dBaseline = read.csv('BaselineAssessmentFYP.csv')
d = merge(d, dBaseline, by = 'ID')
# Merge Essay Coding
dEssays = read.csv('FYPEssays.csv')
d = merge(d, dEssays, by = 'ID')
# ::: Add Number of Math Classes Taken
dMathClasses = read.csv('MathClasses.csv')
d = merge(d, dMathClasses, by.x = 'ID')
#### Creating Variables for Regressions ####
d$Diligence = 240 - (d$DT/1000)
d$totalScoreAdjusted = d$totalScore - d$testTotalNI
dNoOut = filter(d, totalScoreAdjusted > -10)
# : Z Scoring PCM for Interactions
d$PCM_Z = (d$PCMTotal - mean(d$PCMTotal))/sd(d$PCMTotal)
# : Centering BaselineScore for Interpretability
d$BaselineScore_C = d$BaselineScore - mean(d$BaselineScore)
d$Practice1Clicks_Z = scale(d$Practice1Clicks) %>% as.numeric()
# Making Factors Numeric so I can test when they equals 0
d$Female_num = ifelse(d$Female=='Female',.5,-.5)
d$UV_num = ifelse(d$UV_v_No=='UV',.5,-.5)
d$FB_num = ifelse(d$FB_v_No=='Feedback',.5,-.5)
# Standardizing Psychological Outcomes
d$UVpostVidTotal_Z = (d$UVpostVidTotal - mean(d$UVpostVidTotal))/sd(d$UVpostVidTotal)
d$ExpPostVidTotal_Z = (d$ExpPostVidTotal - mean(d$ExpPostVidTotal))/sd(d$ExpPostVidTotal)
d$UVpostWriTotal_Z = (d$UVpostWriTotal - mean(d$UVpostWriTotal))/sd(d$UVpostWriTotal)
d$UVfinalTotal_Z = (d$UVfinalTotal - mean(d$UVfinalTotal))/sd(d$UVfinalTotal)
d$ExpPostWriTotal_Z = (d$ExpPostWriTotal - mean(d$ExpPostWriTotal))/sd(d$ExpPostWriTotal)
d$UVfinalTotal_Z = (d$UVfinalTotal - mean(d$UVfinalTotal))/sd(d$UVfinalTotal)
d$SIfinalTotal_Z = (d$SIfinalTotal - mean(d$SIfinalTotal))/sd(d$SIfinalTotal)
d$CurrentGPA_Z = (d$CurrentGPA - mean(d$CurrentGPA))/sd(d$CurrentGPA)
d$SIpostVidTotal_Z = (d$SIpostVidTotal - mean(d$SIpostVidTotal))/sd(d$SIpostVidTotal)
#### Figuring Out Best Measure of Dilligence ####
d$DT_z = scale(d$DT) %>% as.numeric()
d$BreakTaken_c = d$BreakTaken - .5
d$Effort_z = scale(d$EffortStudying) %>% as.numeric()
d$NumAtt_z = scale(d$NumAtt) %>% as.numeric()
d$NC_z = scale(d$NC) %>% as.numeric()
d$BaselineScore_z = scale(d$BaselineScore) %>% as.numeric()
#### Final AERA Regressions ####
contrasts(d$Female)  = varContrasts(d$Female, Type = 'POC', POCList = list(c(.5,-.5)), Labels = 'Female_vs_Male')
d$BaselineScore_Z = scale(d$BaselineScore) %>% as.numeric()
## SI
m1 = lm(SIpostWriTotal ~ Female + BaselineScore_Z + SIpostVidTotal_Z + ExpPostVidTotal_Z + UV_v_No*Female, data = d)
# Exp
m1 = lm(ExpPostWriTotal ~ Female, data = d)
m2 = lm(ExpPostWriTotal ~ Female + PCM_Z + BaselineScore_Z, data = d)
m3 = lm(ExpPostWriTotal ~ Female + PCM_Z + BaselineScore_Z + SIpostVidTotal_Z + UVpostVidTotal_Z, data = d)
m4 = lm(ExpPostWriTotal ~ Female + PCM_Z + BaselineScore_Z + SIpostVidTotal_Z + UVpostVidTotal_Z + ExpPostVidTotal_Z, data = d)
m5 = lm(ExpPostWriTotal ~ Female + PCM_Z + BaselineScore_Z + SIpostVidTotal_Z + UVpostVidTotal_Z + ExpPostVidTotal_Z + UV_v_No*Female, data = d)
m6 = lm(ExpPostWriTotal ~ Female + PCM_Z + BaselineScore_Z + SIpostVidTotal_Z + UVpostVidTotal_Z + ExpPostVidTotal_Z + UV_v_No*Female + UV_v_No*ExpPostVidTotal_Z, data = d)
judyTable(m1,m2,m3,m4,m5,m6)
setwd(packdir)
vTable(m1)
vTable(m2)
vTable(m1,m2,m3)
devtools::load_all()
devtools::load_all()
vTable(m1,m2,m3)
devtools::load_all()
vTable(m1,m2,m3)
vTable(m1,m2,m3)
devtools::load_all()
vTable(m1,m2,m3)
vTable(m1,m2,m3)
vTable(m1,m2,m3)
devtools::load_all()
vTable(m1,m2,m3)
vTable(m1)
vTable(m1,m2,m3)
?vTable
devtools::document()
vTable(m1,m2,m3)
library(car)
??car
mtcars
devtools::document()
devtools::load_all()
?Asher::stepTable()
devtools::document()
devtools::load_all()
?Asher::stepTable()
